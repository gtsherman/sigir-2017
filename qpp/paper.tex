\documentclass{sig-alternate}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{verbatim}
\usepackage{colortbl}
\usepackage[table]{xcolor}

\newcommand\rankeq{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily rank}}}{=}}}
\newcommand{\bigcell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\setlist[itemize]{noitemsep}  % remove space between list items
\setlist[enumerate]{noitemsep}  % remove space between list items

\begin{document}

\author{[Blind]}

\title{Pseudo-Relevance Query Performance Prediction for Information Retrieval}

\maketitle
\begin{abstract}
\end{abstract}

\section{Introduction}\label{section.intro}

Query performance prediction (QPP) is the task of estimating the success of an information retrieval (IR) system in retrieving results for a submitted query. Prediction strategies generally use properties of the query or of the retrieved document set to quantify the success of a system. The former is called pre-retrieval prediction, while the latter is called post-retrieval prediction. On the whole, post-retrieval prediction outperforms pre-retrieval prediction, and our predictor belongs to this class.

In general, post-retrieval prediction methods tend to associate some property of the result set with the relevance of the result set in general (or, more accurately, some measurement thereof). These properties are usually intuitively suggestive of some form of stability or robustness, as discussed in Section \ref{section.related}. 

In this work, we propose a predictor inspired by pseudo-relevance feedback techniques that directly incorporates relevance metrics in the prediction process. We accomplish this by asserting that the top $r$ retreived documents in a ranked list for a query $Q$ are relevant to that query; we then use these pseudo-relevance judgments to estimate document retrieval evaluation using natural variations on the query: the top $n$ ranked documents. 

\section{Related Work}\label{section.related}

A great deal of research has focused on QPP \cite{Carmel2010}. The most famous predictor is query clarity (QC) \cite{Cronen-Townsend2002}. Clarity is simply the KL-divergence of an expanded query language model and the collection as a whole:

\begin{flalign}\label{eq.clarity}
\sum_{w \in V} P(w|Q) \log \frac{P(w|Q)}{P(w|C)}
\end{flalign}

\noindent where $w$ is a term in the vocabulary $V$, $P(w|C)$ is the maximum likelihood estimate of $w$ in collection $C$, and $P(w|Q)$ is the probability of $w$ as computed by a relevance model \cite{Lavrenko2001}. 

%The latter are computed by

% \begin{flalign}\label{eq.rm}
% \sum_{D \in C} P(w|D) P(Q|D).
% \end{flalign}
% 
% \noindent $P(Q|D)$ is computed as in Eq. 

The idea underlying clarity is that a query whose top ranked documents are linguistically distinct from a generic background language model has likely done a better job identifying relevant documents than one whose top ranked documents resemble the background model. Several modifications of query clarity have been proposed, e.g. \cite{Diaz2004, Hauff2008, He2004}. We incorporate traditional query clarity as a baseline in our work.

One class of query performance predictor that is related to our work is query perturbation \cite{Vinay2006, Yom-Tov2005, Zhou2007}. These methods alter the query and measure the similarity of the results list across query variations; if the result list remains relatively constant, the system is believed to have little difficulty with the query. Our proposed method is fundamentally a form of query perturbation, with the alternative query forms derived from feedback documents.

Zhou \& Croft's query feedback (QF) technique constructs a new query from the top ranked documents \cite{Zhou2007}. First, QF issues $Q$ against collection $C$, retrieving a results list $L$. By ranking each term $w$ in $L$ according to its contribution to query clarity (that is, its contribution to the KL-divergence with the collection) and selecting the top ranked terms, a new query $Q'$ is constructed. After retrieving results list $L'$ for $Q'$, the percentage overlap between $L$ and $L'$ is computed. This overlap is the QF score. This technique bears a resemblance to our proposed method: like ours, the top ranked results give rise to alternate queries, and the results lists of the original and alternate queries are compared. Unlike ours, however, QF produces only a single alternate query; our method also differs in its scoring component, which directly models retrieval performance rather than simply measuring overlap as QF does. Because of its similarity to our method, we use query feedback as a second baseline.

\section{Pseudo-Relevance Performance Predictor}\label{section.method}

\subsection{Underlying Retrieval Model}\label{section.method.underlying}

In this paper, we employ the standard multinomial query likelihood (QL) retrieval model to rank the relevance of a document $D$ to query $Q$, although it should be noted that this model is not required by our query performance predictor.

Assuming a uniform distribution over documents, QL scores $D$ by the following:

\begin{flalign}\label{eq.ql}
P(Q|D) = \prod_{i=1}^{|Q|} P(q_i|\theta_D)
\end{flalign}

\noindent where $q_i$ is a word in $Q$ and $P(q_i|\theta_D)$ is estimated using Dirichlet smoothing:

\begin{flalign}\label{eq.dirichlet}
P(q_i|\theta_D) = \frac{c(q_i,D) + \mu \hat{P}(q_i|C)}{|D| + \mu}
\end{flalign}

\noindent where $c(q_i,D)$ is the frequency of $q_i$ in $D$, $|D|$ is the length of the document, $\hat{P}(q_i|C)$ is the maximum likelihood estimate of $q_i$ in collection $C$, and $\mu$ is a parameter. We set $\mu = 2500$ for all runs.

\subsection{Predicting Query Performance}\label{section.method.predicting}

Our query performance predictor, which we call the pseudo-relevance performance predictor (PRPP), is inspired by the pseudo-relevance feedback technique of treating the top ranked documents as relevant to the query. Doing so allows us to directly model the robustness of query performance metrics to query perturbation. The broad steps of our approach are as follows:

\begin{enumerate}
	\item Issue query $Q$ against collection $C_p$; call the results list $R_p$
	\item Record the top $r$ documents in $R_p$ as relevant using each's normalized retrieval score as the degree of relevance
	\item Issue $Q$ against target collection $C_t$, if $C_p \neq C_t$; call the results list $R_t$
	\item For each document $D$ in $R_t$,
	\begin{enumerate}
		\item Construct a pseudo-query $Q_D$ consisting of the top $q$ non-stop terms\footnote{We remove terms in the standard Indri stoplist: http://www.lemurproject.org/stopwords/stoplist.dft} in $D$
		\item Issue $Q_D$ against $C_p$; call the results list $R_{pD}$
		\item Score $R_{pD}$ using the pseudo-relevance judgments from Step 2.
	\end{enumerate}
	\item Average the scores to compute a single score for $Q$
\end{enumerate}

\noindent Several points in the preceding would benefit from clarification.

First, our model makes no assertions about the identity of the performance collection $C_p$. While it is valid to use the target collection as the performance collection, we hypothesize that the use of an external collection may yield better results. We investigate this idea in Section \ref{section.experiments}.

Next, both $r$ and $q$ are free parameters in our approach. In fact, there are two other parameters in our approach: $k$ controls the length of $R_t$, while $n$ controls the length of $R_{pD}$. In general, all parameters should be set empirically. Although not required mathematically, we only allow values of $n$ where $n \leq r$ since setting $n$ greater than $r$ prevents us from retrieving all relevant documents.

One advantage to our model is the ability to predict specific retrieval metrics by estimating levels of relevance. Because these levels are normalized log probabilities, they consist of continuous values (in contrast to TREC qrels which contain binary or discrete relevance grades), but are still valid for metrics that incorporate graded relevance judgments like normalized discounted cumulative gain (nDCG). Metrics that use only binary relevance metrics, like mean average precision (MAP) follow the typical procedure of treating all judgments greater than zero as relevant. 

\section{Evaluation}\label{section.evaluation}

\subsection{Data}\label{section.evaluation.data}

We evaluate our procedure using a variety of TREC datasets that represent a variety of corpora properties:

\begin{itemize}
	\item AP newsire with topics 101-200 from TREC disks 1 and 2
	\item Robust with the 2004 topics from TREC disks 4 and 5
	\item wt10g with the topics 451-500 from the 2000 and 2001 TREC Web tracks
\end{itemize}

We also make use of Wikipedia\footnote{http://en.wikipedia.org} as an external collection in some experiments.

% \subsection{Parameters}\label{section.evaluation.runs}

% Our predictor requires the setting of several parameters. We set $r$, $k$, and $n$ by 10-fold cross-validation: we sweep over each parameter at the values $\{1, 5, 10, 20, 50, 75, 100\}$; as noted in Section \ref{section.method}, $n$ is only swept at values $\leq r$. We set $q = 20$ for all runs since using the entire length of $D$, $|D|$, is excessively expensive during retrieval.

% We follow a typical cross-validation procedure, as follows. We split the batch of test queries into ten folds. All runs use the same random division of queries to ensure fair comparisons. For each fold, we evaluate the training queries at each parameter setting and determine which yields the highest correlation with the target metric (e.g. MAP or nDCG). We then apply the optimal parameter setting to the test queries. By concatenating each fold of test queries, we produce a single test run comprising all queries in the batch, on which we can then run a final correlation to evaluate our predictor.

% We perform the same procedure on the parameters involved in our baseline predictors, which are discussed in more detail in Section \ref{section.results}.

\section{Results}\label{section.results}

% \begin{table}
% \begin{tabular}{|l|l|c|c|c|c|} \hline
% & & \multicolumn{2}{c|}{MAP} & \multicolumn{2}{c|}{nDCG@20} \\ \cline{3-6}
% Corpus & Pred. & Med. & Max & Med. & Max \\ \hline\hline
% \multirow{4}{*}{AP} & QC & \textbf{0.5280} & \textbf{0.6148} & 0.3564 & 0.4284 \\ \cline{2-6}
% & QF & 0.4240 & 0.5637 & 0.3202 & 0.4467 \\ \cline{2-6}
% & PRPP-flip & 0.5020 & 0.5661 & \textbf{0.4026} & \textbf{0.5092} \\ \cline{2-6}
% & PRPP & 0.5030 & 0.6115 & 0.4025 & 0.4880 \\ \hline\hline
% \multirow{4}{*}{wt10g} & QC & 0.2232 & 0.3636 & 0.0943 & 0.3436 \\ \cline{2-6}
% & QF & 0.1624 & 0.3847 & 0.0881 & 0.3613 \\ \cline{2-6}
% & PRPP-flip & 0.2910 & 0.4665 & 0.2367 & 0.3949 \\ \cline{2-6}
% & PRPP & \textbf{0.2850} & \textbf{0.4615} & \textbf{0.2529} & \textbf{0.4173} \\ \hline\hline
% \multirow{4}{*}{Robust} & QC & 0.4240 & 0.5114 & 0.3129 & 0.3914 \\ \cline{2-6}
% & QF & 0.2203 & 0.5073 & 0.1708 & 0.4926 \\ \cline{2-6}
% & PRPP-flip & 0.3826 & 0.5743 & 0.3540 & 0.5094 \\ \cline{2-6}
% & PRPP & \textbf{0.4381} & \textbf{0.5791} & \textbf{0.3222} & \textbf{0.5047} \\ \hline
% \end{tabular}
% \caption{Median and maximum Pearson's $r$ correlation of predictors for each collection. Bolded values are the largest observed per corpus/metric summary.}
% \label{table.results.self.pearson}
% \end{table}
% 
% \begin{table}
% \begin{tabular}{|l|l|c|c|c|c|} \hline
% & & \multicolumn{2}{c|}{MAP} & \multicolumn{2}{c|}{nDCG@20} \\ \cline{3-6}
% Corpus & Pred. & Med. & Max & Med. & Max \\ \hline\hline
% \multirow{4}{*}{AP} & QC & 0.3609 & 0.4440 & 0.2166 & 0.2807 \\ \cline{2-6}
% & QF & 0.3259 & 0.4215 & 0.2042 & 0.3100 \\ \cline{2-6}
% & PRPP-flip & 0.4068 & 0.4670 & \textbf{0.2769} & \textbf{0.3543} \\ \cline{2-6}
% & PRPP & \textbf{0.4132} & \textbf{0.4962} & 0.2701 & 0.3378 \\ \hline\hline
% \multirow{4}{*}{wt10g} & QC & 0.1561 & 0.3093 & 0.0712 & 0.2622 \\ \cline{2-6}
% & QF & 0.1599 & 0.2843 & 0.0657 & 0.2411 \\ \cline{2-6}
% & PRPP-flip & 0.2586 & 0.3772 & 0.2006 & 0.2844 \\ \cline{2-6}
% & PRPP & \textbf{0.2710} & \textbf{0.3642} & \textbf{0.1935} & \textbf{0.3053} \\ \hline\hline
% \multirow{4}{*}{Robust} & QC & 0.2985 & 0.3714 & 0.2266 & 0.2889 \\ \cline{2-6}
% & QF & 0.1895 & 0.3745 & 0.1389 & \textbf{0.3646} \\ \cline{2-6}
% & PRPP-flip & 0.3082 & 0.4148 & 0.2783 & 0.3698 \\ \cline{2-6}
% & PRPP & \textbf{0.3433} & \textbf{0.4273} & \textbf{0.2478} & 0.3634 \\ \hline
% \end{tabular}
% \caption{Median and maximum Kendall's $\tau$ correlation of predictors for each collection. Bolded values are the largest observed per corpus/metric summary.}
% \label{table.results.self.kendall}
% \end{table}

\begin{table}
\begin{tabular}{|l|l|c|c|c|c|} \hline
& & \multicolumn{2}{c|}{MAP} & \multicolumn{2}{c|}{nDCG@20} \\ \cline{3-6}
Corpus & Pred. & Valid. & Max. & Valid. & Max \\ \hline\hline
\multirow{4}{*}{AP} & QC & 0.4748 & 0.6148 & 0.2488* & 0.4284 \\ \cline{2-6}
& QF & 0.5178 & 0.5637 & 0.2368* & 0.4467 \\ \cline{2-6}
& PRPP-flip & 0.4481 & 0.5661 & 0.4334 & 0.5092 \\ \cline{2-6}
& PRPP & 0.5003 & 0.6115 & 0.3848 & 0.4880 \\ \hline\hline
\multirow{4}{*}{wt10g} & QC & 0.2873 & 0.3636 & 0.2271* & 0.3436 \\ \cline{2-6}
& QF & 0.3758 & 0.3847 & 0.3937 & 0.3613 \\ \cline{2-6}
& PRPP-flip & 0.2758* & 0.4665 & 0.0957* & 0.3949 \\ \cline{2-6}
& PRPP & 0.2533* & 0.4615 & -0.0181* & 0.4173 \\ \hline\hline
\multirow{4}{*}{Robust} & QC & 0.4534 & 0.5114 & 0.3768 & 0.3914 \\ \cline{2-6}
& QF & 0.5061 & 0.5073 & 0.4884 & 0.4926 \\ \cline{2-6}
& PRPP-flip & 0.5581 & 0.5743 & 0.4889 & 0.5094 \\ \cline{2-6}
& PRPP & 0.5602 & 0.5791 & 0.4986 & 0.5047 \\ \hline
\end{tabular}
\caption{Median and maximum Pearson's $r$ correlation of predictors for each collection. Bolded values are the largest observed per corpus/metric summary.}
\label{table.results.self.pearson}
\end{table}

\begin{table}
\begin{tabular}{|l|l|c|c|c|c|} \hline
& & \multicolumn{2}{c|}{MAP} & \multicolumn{2}{c|}{nDCG@20} \\ \cline{3-6}
Corpus & Pred. & Med. & Max & Med. & Max \\ \hline\hline
\multirow{4}{*}{AP} & QC & & & & \\ \cline{2-6}
& QF & & & & \\ \cline{2-6}
& PRPP-flip & & & & \\ \cline{2-6}
& PRPP & & & & \\ \hline\hline
\multirow{4}{*}{wt10g} & QC & & & & \\ \cline{2-6}
& QF & & & & \\ \cline{2-6}
& PRPP-flip & & & & \\ \cline{2-6}
& PRPP & & & & \\ \hline\hline
\multirow{4}{*}{Robust} & QC & & & & \\ \cline{2-6}
& QF & & & & \\ \cline{2-6}
& PRPP-flip & & & & \\ \cline{2-6}
& PRPP & & & & \\ \hline
\end{tabular}
\caption{Median and maximum Kendall's $\tau$ correlation of predictors for each collection. Bolded values are the largest observed per corpus/metric summary.}
\label{table.results.self.kendall}
\end{table}

The results of our predictor as well as the two baselines are shown in Tables \ref{table.results.self.pearson} and \ref{table.results.self.kendall}; the former displays Pearson's $r$ correlation while the latter displays Kendall's $\tau$ rank correlation.

\section{Conclusions}\label{section.conclusions}

\bibliographystyle{abbrv}
\bibliography{library}  

\end{document}
