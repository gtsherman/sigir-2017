\documentclass{sig-alternate}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{verbatim}
\usepackage{colortbl}
\usepackage[table]{xcolor}

\newcommand\rankeq{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily rank}}}{=}}}
\newcommand{\bigcell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\setlist[itemize]{noitemsep}  % remove space between list items
\setlist[enumerate]{noitemsep}  % remove space between list items

\begin{document}

\author{Garrick Sherman}

\title{}

\maketitle
\begin{abstract}
\end{abstract}

\section{Introduction}\label{section.intro}

Query performance prediction (QPP) is the task of estimating the success of an information retrieval (IR) system in retrieving results for a submitted query. Prediction strategies generally use properties of the query or of the retrieved document set to quantify the success of a system. The former is called pre-retrieval prediction, while the latter is called post-retrieval prediction. On the whole, post-retrieval prediction outperforms pre-retrieval prediction, and our predictor belongs to this class.

In general, post-retrieval prediction methods tend to associate some property of the result set with the relevance of the result set in general (or, more accurately, some measurement thereof). These properties are usually intuitively suggestive of some form of stability or robustness, as discussed in Section \ref{section.related}. 

In this work, we propose a predictor inspired by pseudo-relevance feedback techniques that directly incorporates relevance metrics in the prediction process. We accomplish this by asserting that the top $r$ retreived documents in a ranked list for a query $Q$ are relevant to that query; we then use these pseudo-relevance judgments to estimate document retrieval evaluation using natural variations on the query: the top $n$ ranked documents. 

\section{Related Work}\label{section.related}

A great deal of research has focused on QPP. The most famous predictor is query clarity, which is simply the KL-divergence of an expanded query model and the collection as a whole \cite{Cronen-Townsend2002}. We incorporate query clarity as a baseline in our work.

One class of query performance predictor that is related to our work is query perturbation \cite{Vinay2006, Yom-Tov2005, Zhou2007}. These methods alter the query and measure the similarity of the results list across query variations; if the result list remains relatively constant, the system is believed to have little difficulty with the query. Our proposed method is fundamentally a form of query perturbation, with the alternative query forms derived from feedback documents.

\section{Pseudo-Relevance Performance Predictor}\label{section.method}

Our query performance predictor, which we call the pseudo-relevance performance predictor (PRPP), is inspired by the pseudo-relevance feedback technique of treating the top ranked documents as relevant to the query. Doing so allows us to directly model the robustness of query performance metrics to query perturbation. The broad steps of our approach are as follows:

\begin{enumerate}
	\item Issue query $Q$ against collection $C_p$; call the results list $R_p$
	\item Record the top $r$ documents in $R_p$ as relevant using each's normalized retrieval score as the degree of relevance
	\item Issue $Q$ against target collection $C_t$, if $C_p \neq C_t$; call the results list $R_t$
	\item For each document $D$ in $R_t$,
	\begin{enumerate}
		\item Construct a pseudo-query $Q_D$ consisting of the top $q$ non-stop terms\footnote{We remove terms in the standard Indri stoplist: http://www.lemurproject.org/stopwords/stoplist.dft} in $D$
		\item Issue $Q_D$ against $C_p$; call the results list $R_{pD}$
		\item Score $R_{pD}$ using the pseudo-relevance judgments from Step 2.
	\end{enumerate}
	\item Average the scores to compute a single score for $Q$
\end{enumerate}

\noindent Several points in the preceding would benefit from clarification.

First, our model makes no assertions about the identity of the performance collection $C_p$. While it is valid to use the target collection as the performance collection, we hypothesize that the use of an external collection may yield better results. We investigate this idea in Section \ref{section.experiments}.

Next, both $r$ and $q$ are free parameters in our approach. In fact, there are two other parameters in our approach: $k$ controls the length of $R_t$, while $n$ controls the length of $R_{pD}$. In general, all parameters should be set empirically. Although not required mathematically, we only allow values of $n$ where $n \leq r$ since setting $n$ greater than $r$ prevents us from retrieving all relevant documents.

One advantage to our model is the ability to predict specific retrieval metrics by estimating levels of relevance. Because these levels are normalized log probabilities, they consist of continuous values (in contrast to TREC qrels which contain binary or discrete relevance grades), but are still valid for metrics that incorporate graded relevance judgments like normalized discounted cumulative gain (nDCG). Metrics that use only binary relevance metrics, like mean average precision (MAP) follow the typical procedure of treating all judgments greater than zero as relevant. 

\section{Evaluation}\label{section.evaluation}

\subsection{Data}\label{section.evaluation.data}

We evaluate our procedure using a variety of TREC datasets that represent a variety of corpora properties:

\begin{itemize}
	\item AP newsire with topics 101-200 from TREC disks 1 and 2
	\item Robust with the 2004 topics from TREC disks 4 and 5
	\item wt10g with the topics 451-500 from the 2000 and 2001 TREC Web tracks
\end{itemize}

We also make use of Wikipedia\footnote{http://en.wikipedia.org} as an external collection in some experiments.

% \subsection{Parameters}\label{section.evaluation.runs}

% Our predictor requires the setting of several parameters. We set $r$, $k$, and $n$ by 10-fold cross-validation: we sweep over each parameter at the values $\{1, 5, 10, 20, 50, 75, 100\}$; as noted in Section \ref{section.method}, $n$ is only swept at values $\leq r$. We set $q = 20$ for all runs since using the entire length of $D$, $|D|$, is excessively expensive during retrieval.

% We follow a typical cross-validation procedure, as follows. We split the batch of test queries into ten folds. All runs use the same random division of queries to ensure fair comparisons. For each fold, we evaluate the training queries at each parameter setting and determine which yields the highest correlation with the target metric (e.g. MAP or nDCG). We then apply the optimal parameter setting to the test queries. By concatenating each fold of test queries, we produce a single test run comprising all queries in the batch, on which we can then run a final correlation to evaluate our predictor.

% We perform the same procedure on the parameters involved in our baseline predictors, which are discussed in more detail in Section \ref{section.results}.

\section{Results}\label{section.results}

% \begin{table}
% \begin{tabular}{|l|l|c|c|c|c|} \hline\hline
% & & \multicolumn{2}{c|}{MAP} & \multicolumn{2}{c|}{nDCG@20} \\ \hline\hline
% Corpus & Predictor & Pearson & Kendall & Pearson & Kendall \\ \hline\hline
% \multirow{3}{*}{AP} & Query clarity & 0.3936 & \textbf{0.3627} & 0.1267 & 0.1194 \\ \cline{2-6}
% & Query feedback & 0.4244 & 0.3010 & \textbf{0.4467} & 0.3100 \\ \cline{2-6}
% & PRPP & \textbf{0.5575} & 0.2904 & 0.1720 & \textbf{0.3110} \\ \hline\hline
% \multirow{3}{*}{wt10g} & Query clarity & 0.2206 & 0.2606 & 0.3436 & 0.2622 \\ \cline{2-6}
% & Query feedback & 0.3847 & 0.2327 & 0.2921 & 0.1860 \\ \cline{2-6}
% & PRPP & -0.0132 & 0.3307 & 0.1695 & 0.2046 \\ \hline\hline
% \multirow{3}{*}{Robust} & Query clarity & 0.4451 & 0.3681 & 0.2427 & 0.2821 \\ \cline{2-6}
% & Query feedback & 0.4685 & 0.3112 & 0.4526 & 0.3073 \\ \cline{2-6}
% & PRPP & \textbf{0.5438} & 0.3874 & 0.2232 & 0.3323 \\ \hline\hline
% \end{tabular}
% \caption{Performance of predictors with Pearson's $r$ and Kendall's $\tau$ correlation.}
% \label{table.results.self}
% \end{table}
% 
% \begin{table}
% \begin{tabular}{|l|c|c|c|c|} \hline\hline
% & \multicolumn{2}{c|}{MAP} & \multicolumn{2}{c|}{nDCG@20} \\ \hline\hline
% Predictor & Pearson & Kendall & Pearson & Kendall \\ \hline\hline
% Query clarity &  &  &  &  \\ \hline\hline
% Query feedback &  &  &  &  \\ \hline\hline
% PRPP &  &  &  &  \\ \hline\hline
% \end{tabular}
% \caption{Performance of predictors with Pearson's $r$ and Kendall's $\tau$ correlation using Wikipedia.}
% \label{table.results.self}
% \end{table}

\begin{table}
\begin{tabular}{|l|l|c|c|c|c|} \hline
& & \multicolumn{2}{c|}{MAP} & \multicolumn{2}{c|}{nDCG@20} \\ \cline{3-6}
Corpus & Pred. & Med. & Max & Med. & Max \\ \hline\hline
\multirow{3}{*}{AP} & QC & \textbf{0.5280} & \textbf{0.6148} & 0.3564 & 0.4284 \\ \cline{2-6}
& QF & 0.4240 & 0.5637 & 0.3202 & 0.4467 \\ \cline{2-6}
& PRPP & 0.5030 & 0.6115 & \textbf{0.4025} & \textbf{0.4880} \\ \hline\hline
\multirow{3}{*}{wt10g} & QC & 0.2232 & 0.3636 & 0.0943 & 0.3436 \\ \cline{2-6}
& QF & 0.1624 & 0.3847 & 0.0881 & 0.3613 \\ \cline{2-6}
& PRPP & \textbf{0.2850} & \textbf{0.4615} & \textbf{0.2529} & \textbf{0.4173} \\ \hline\hline
\multirow{3}{*}{Robust} & QC & 0.4240 & 0.5114 & 0.3129 & 0.3914 \\ \cline{2-6}
& QF & 0.2203 & 0.5073 & 0.1708 & 0.4926 \\ \cline{2-6}
& PRPP & \textbf{0.4381} & \textbf{0.5791} & \textbf{0.3222} & \textbf{0.5047} \\ \hline
\end{tabular}
\caption{Median and maximum Pearson's $r$ correlation of predictors for each collection. Bolded values are the largest observed per corpus/metric summary.}
\label{table.results.self.pearson}
\end{table}

\begin{table}
\begin{tabular}{|l|l|c|c|c|c|} \hline
& & \multicolumn{2}{c|}{MAP} & \multicolumn{2}{c|}{nDCG@20} \\ \cline{3-6}
Corpus & Pred. & Med. & Max & Med. & Max \\ \hline\hline
\multirow{3}{*}{AP} & QC & 0.3609 & 0.4440 & 0.2166 & 0.2807 \\ \cline{2-6}
& QF & 0.3259 & 0.4215 & 0.2042 & 0.3100 \\ \cline{2-6}
& PRPP & \textbf{0.4132} & \textbf{0.4962} & \textbf{0.2701} & \textbf{0.3378} \\ \hline\hline
\multirow{3}{*}{wt10g} & QC & 0.1561 & 0.3093 & 0.0712 & 0.2622 \\ \cline{2-6}
& QF & 0.1599 & 0.2843 & 0.0657 & 0.2411 \\ \cline{2-6}
& PRPP & \textbf{0.2710} & \textbf{0.3642} & \textbf{0.1935} & \textbf{0.3053} \\ \hline\hline
\multirow{3}{*}{Robust} & QC & 0.2985 & 0.3714 & 0.2266 & 0.2889 \\ \cline{2-6}
& QF & 0.1895 & 0.3745 & 0.1389 & \textbf{0.3646} \\ \cline{2-6}
& PRPP & \textbf{0.3433} & \textbf{0.4273} & \textbf{0.2478} & 0.3634 \\ \hline
\end{tabular}
\caption{Median and maximum Kendall's $\tau$ correlation of predictors for each correlation. Bolded values are the largest observed per corpus/metric summary.}
\label{table.results.self.kendall}
\end{table}



\section{Conclusions}\label{section.conclusions}

\section{Acknowledgments}\label{section.acknowledgments}

This work was supported in part by the US National Science Foundation under Grant No. [blind]. Any opinions, findings, conclusions, or recommendations expressed are those of the authors and do not necessarily reflect the views of the National Science Foundation.

\bibliographystyle{abbrv}
\bibliography{library}  

\end{document}
